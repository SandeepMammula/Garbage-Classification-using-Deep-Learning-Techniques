# -*- coding: utf-8 -*-
"""InceptionResnetV2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D_uElvKgK8ZOAKu5097VOXW8gZ2J11xZ
"""

!nvidia-smi

from google.colab import drive

drive.mount("/content/drive/")

import tensorflow as tf
from keras.layers import Input, Lambda, Dense, Flatten
from keras.models import Model
from keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
import numpy as np
from glob import glob
import matplotlib.pyplot as plt
tf.keras.applications.inception_resnet_v2.preprocess_input

base_model = tf.keras.applications.InceptionResNetV2(
    include_top=False,
    weights="imagenet",
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=6,
    classifier_activation="softmax",
)

dataset_train = '/content/drive/MyDrive/garbage_dataset/garbage_dataset/train'
dataset_test = '/content/drive/MyDrive/garbage_dataset/garbage_dataset/test'

import os

# Function to count files in a directory
def count_files(directory):
    return len([filename for filename in os.listdir(directory) if os.path.isfile(os.path.join(directory, filename))])

# Iterate over each folder and count the number of images
image_counts = {}
for folder in os.listdir(dataset_train):
    folder_path = os.path.join(dataset_train, folder)
    if os.path.isdir(folder_path):
        image_counts[folder] = count_files(folder_path)

# Print the results
print("No. of images in each folder for training-->")
total_count = 0
for folder, count in image_counts.items():
    print(f"{folder}: {count} images")
    total_count += count
print('Length of training dataset: {}' .format(total_count))

# Function to count files in a directory
def count_files(directory):
    return len([filename for filename in os.listdir(directory) if os.path.isfile(os.path.join(directory, filename))])

# Iterate over each folder and count the number of images
image_counts = {}
for folder in os.listdir(dataset_test):
    folder_path = os.path.join(dataset_test, folder)
    if os.path.isdir(folder_path):
        image_counts[folder] = count_files(folder_path)

# Print the results
print("No. of images in each folder for testing-->")
total_count = 0
for folder, count in image_counts.items():
    print(f"{folder}: {count} images")
    total_count += count
print('Length of test dataset: {}' .format(total_count))

from keras.applications.inception_v3 import InceptionV3
from keras.models import Model
from keras.layers import Dense, GlobalAveragePooling2D

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
predictions = Dense(6, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=predictions)

for layer in base_model.layers:
    layer.trainable = False

model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

from keras.preprocessing.image import ImageDataGenerator

# ImageDataGenerator for training data with augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    brightness_range=[0.2, 0.5],
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    vertical_flip=True
)

# ImageDataGenerator for test data (only rescaling)
test_datagen = ImageDataGenerator(rescale=1./255)

# Flow training images in batches using train_datagen generator
train_generator = train_datagen.flow_from_directory(
    dataset_train,
    target_size=(224, 224),
    batch_size=5,
    class_mode='categorical',
    shuffle=True
)

# Flow validation images in batches using test_datagen generator
test_generator = test_datagen.flow_from_directory(
    dataset_test,
    target_size=(224, 224),
    batch_size=5,
    class_mode='categorical',
    shuffle=False
)

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

import subprocess

# Execute shell command to remove the ./logs/ directory
subprocess.run(["rm", "-rf", "./logs/"])

from datetime import datetime
from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint

start = datetime.now()

# Define TensorBoard callback
log_dir = "logs/fit/" + datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

# Define ModelCheckpoint callback to save the best model based on training loss and accuracy
checkpoint_callback = ModelCheckpoint(filepath="./best_model.keras",
                                      monitor='accuracy',  # Monitor training loss
                                      save_best_only=True,  # Save the best model
                                      mode='max',  # Save the model with minimum loss
                                      verbose=1)  # Show progress

history = model.fit(
    train_generator,
    epochs=100,
    steps_per_epoch=10,
    validation_data=test_generator,
    callbacks=[tensorboard_callback, checkpoint_callback]
)

duration = datetime.now() - start
print("Training completed in time: ", duration)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/fit



















