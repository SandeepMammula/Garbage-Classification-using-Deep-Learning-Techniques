# -*- coding: utf-8 -*-
"""Xception.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IojF39isiCc_NBC3SL-0io2lcpFWWlGW
"""

!nvidia-smi

from google.colab import drive

drive.mount("/content/drive/")

import tensorflow as tf
from keras.layers import Input, Lambda, Dense, Flatten
from keras.models import Model
from keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
import numpy as np
from glob import glob
import matplotlib.pyplot as plt
tf.keras.applications.xception.preprocess_input

base_model = tf.keras.applications.Xception(
    include_top=False,
    weights="imagenet",
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=6,
    classifier_activation="softmax",
)

dataset_train = '/content/drive/MyDrive/garbage_dataset/garbage_dataset/train'
dataset_test = '/content/drive/MyDrive/garbage_dataset/garbage_dataset/test'

import os

# Function to count files in a directory
def count_files(directory):
    return len([filename for filename in os.listdir(directory) if os.path.isfile(os.path.join(directory, filename))])

# Iterate over each folder and count the number of images
image_counts = {}
for folder in os.listdir(dataset_train):
    folder_path = os.path.join(dataset_train, folder)
    if os.path.isdir(folder_path):
        image_counts[folder] = count_files(folder_path)

# Print the results
print("No. of images in each folder for training-->")
total_count = 0
for folder, count in image_counts.items():
    print(f"{folder}: {count} images")
    total_count += count
print('Length of training dataset: {}' .format(total_count))

# Function to count files in a directory
def count_files(directory):
    return len([filename for filename in os.listdir(directory) if os.path.isfile(os.path.join(directory, filename))])

# Iterate over each folder and count the number of images
image_counts = {}
for folder in os.listdir(dataset_test):
    folder_path = os.path.join(dataset_test, folder)
    if os.path.isdir(folder_path):
        image_counts[folder] = count_files(folder_path)

# Print the results
print("No. of images in each folder for testing-->")
total_count = 0
for folder, count in image_counts.items():
    print(f"{folder}: {count} images")
    total_count += count
print('Length of test dataset: {}' .format(total_count))

from keras.applications.inception_v3 import InceptionV3
from keras.models import Model
from keras.layers import Dense, GlobalAveragePooling2D

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
predictions = Dense(6, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=predictions)

for layer in base_model.layers:
    layer.trainable = False

model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

from keras.preprocessing.image import ImageDataGenerator

# ImageDataGenerator for training data with augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    brightness_range=[0.2, 0.5],
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    vertical_flip=True
)

# ImageDataGenerator for test data (only rescaling)
test_datagen = ImageDataGenerator(rescale=1./255)

# Flow training images in batches using train_datagen generator
train_generator = train_datagen.flow_from_directory(
    dataset_train,
    target_size=(224, 224),
    batch_size=5,
    class_mode='categorical',
    shuffle=True
)

# Flow validation images in batches using test_datagen generator
test_generator = test_datagen.flow_from_directory(
    dataset_test,
    target_size=(224, 224),
    batch_size=5,
    class_mode='categorical',
    shuffle=False
)

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

import subprocess

# Execute shell command to remove the ./logs/ directory
subprocess.run(["rm", "-rf", "./logs/"])

from datetime import datetime
from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint

start = datetime.now()

# Define TensorBoard callback
log_dir = "logs/fit/" + datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

# Define ModelCheckpoint callback to save the best model based on training loss and accuracy
checkpoint_callback = ModelCheckpoint(filepath="./best_model.keras",
                                      monitor='val_accuracy',  # Monitor training loss
                                      save_best_only=True,  # Save the best model
                                      mode='max',  # Save the model with minimum loss
                                      verbose=1)  # Show progress

history = model.fit(
    train_generator,
    epochs=100,
    steps_per_epoch=10,
    validation_data=test_generator,
    callbacks=[tensorboard_callback, checkpoint_callback]
)

duration = datetime.now() - start
print("Training completed in time: ", duration)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/fit

import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import load_model

# Load your trained model
model = load_model('best_model.keras')

# Path to your test dataset folder
dataset_test = '/content/drive/MyDrive/garbage_dataset/garbage_dataset/test'

# Get list of categories (assuming each subfolder represents a category)
categories = sorted(os.listdir(dataset_test))

# Initialize lists to store true labels and predicted labels
true_labels = []
predicted_labels = []

# Iterate through each category folder
for category_index, category in enumerate(categories):
    category_folder = os.path.join(dataset_test, category)
    images = os.listdir(category_folder)

    # Iterate through each image in the category folder
    for image_name in images:
        # Load and preprocess the image
        img_path = os.path.join(category_folder, image_name)
        img = image.load_img(img_path, target_size=(224, 224))  # Adjust target_size as per your model requirements
        img = image.img_to_array(img)
        img = np.expand_dims(img, axis=0)

        # Make prediction using the model
        prediction = model.predict(img)
        predicted_label = np.argmax(prediction)

        # Append true and predicted labels to the lists
        true_labels.append(category_index)
        predicted_labels.append(predicted_label)

# Convert lists to numpy arrays
true_labels = np.array(true_labels)
predicted_labels = np.array(predicted_labels)

# Print classification report
print("Classification Report:")
print(classification_report(true_labels, predicted_labels, target_names=categories))

# Plot confusion matrix
conf_matrix = confusion_matrix(true_labels, predicted_labels)
plt.figure(figsize=(8, 6))
plt.imshow(conf_matrix, cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.colorbar()
plt.xticks(ticks=np.arange(len(categories)), labels=categories, rotation=45)
plt.yticks(ticks=np.arange(len(categories)), labels=categories)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.tight_layout()
plt.show()

















