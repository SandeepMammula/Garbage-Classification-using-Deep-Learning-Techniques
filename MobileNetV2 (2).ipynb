{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5EBW3IUwmCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bfcdcea-f797-4585-be98-40379915fe64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Mar 23 14:39:52 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive/\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCmZOVFaOErK",
        "outputId": "61dbe11d-6185-4377-cd8d-a86aa2b0fcf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Input, Lambda, Dense, Flatten\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "tf.keras.applications.mobilenet_v2.preprocess_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "P0ZcJwUfOGTe",
        "outputId": "524418da-c7ed-4c4f-ce83-186f5e696e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function keras.src.applications.mobilenet_v2.preprocess_input(x, data_format=None)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>keras.src.applications.mobilenet_v2.preprocess_input</b><br/>def preprocess_input(x, data_format=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/keras/src/applications/mobilenet_v2.py</a>Preprocesses a tensor or Numpy array encoding a batch of images.\n",
              "\n",
              "Usage example with `applications.MobileNet`:\n",
              "\n",
              "```python\n",
              "i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)\n",
              "x = tf.cast(i, tf.float32)\n",
              "x = tf.keras.applications.mobilenet.preprocess_input(x)\n",
              "core = tf.keras.applications.MobileNet()\n",
              "x = core(x)\n",
              "model = tf.keras.Model(inputs=[i], outputs=[x])\n",
              "\n",
              "image = tf.image.decode_png(tf.io.read_file(&#x27;file.png&#x27;))\n",
              "result = model(image)\n",
              "```\n",
              "\n",
              "Args:\n",
              "  x: A floating point `numpy.array` or a `tf.Tensor`, 3D or 4D with 3 color\n",
              "    channels, with values in the range [0, 255].\n",
              "    The preprocessed data are written over the input data\n",
              "    if the data types are compatible. To avoid this\n",
              "    behaviour, `numpy.copy(x)` can be used.\n",
              "  data_format: Optional data format of the image tensor/array. None, means\n",
              "    the global setting `tf.keras.backend.image_data_format()` is used\n",
              "    (unless you changed it, it uses &quot;channels_last&quot;).\n",
              "    Defaults to `None`.\n",
              "\n",
              "Returns:\n",
              "    Preprocessed `numpy.array` or a `tf.Tensor` with type `float32`.\n",
              "    \n",
              "    The inputs pixel values are scaled between -1 and 1, sample-wise.\n",
              "\n",
              "Raises:\n",
              "    \n",
              "  ValueError: In case of unknown `data_format` argument.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 573);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MobileNetV2=tf.keras.applications.MobileNetV2(\n",
        "    input_shape= (224, 224, 3),\n",
        "    alpha=1.0,\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    pooling=None,\n",
        "    classes=12,\n",
        "    classifier_activation=\"softmax\",\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "8yMVyt0VOGVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in MobileNetV2.layers:\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "id": "07P5Vx1kOGYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "IMAGE_SIZE = [224, 224]\n",
        "\n",
        "dataset = '/content/drive/MyDrive/garbage_dataset2'\n",
        "\n",
        "\n",
        "#train_ds, test_ds = random_split(dataset, [12412, 3103])\n",
        "#len(train_ds), len(test_ds)"
      ],
      "metadata": {
        "id": "RxNAyudIOGav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dataset_train = '/content/drive/MyDrive/garbage_dataset2/train'\n",
        "dataset_test = '/content/drive/MyDrive/garbage_dataset2/test'\n",
        "\n",
        "# Function to count files in a directory\n",
        "def count_files(directory):\n",
        "    return len([filename for filename in os.listdir(directory) if os.path.isfile(os.path.join(directory, filename))])\n",
        "\n",
        "# Iterate over each folder and count the number of images\n",
        "image_counts = {}\n",
        "for folder in os.listdir(dataset_train):\n",
        "    folder_path = os.path.join(dataset_train, folder)\n",
        "    if os.path.isdir(folder_path):\n",
        "        image_counts[folder] = count_files(folder_path)\n",
        "\n",
        "# Print the results\n",
        "total_count = 0\n",
        "for folder, count in image_counts.items():\n",
        "    print(f\"{folder}: {count} images\")\n",
        "    total_count += count\n",
        "print('Length of training dataset: {}' .format(total_count))\n",
        "\n",
        "# Function to count files in a directory\n",
        "def count_files(directory):\n",
        "    return len([filename for filename in os.listdir(directory) if os.path.isfile(os.path.join(directory, filename))])\n",
        "\n",
        "# Iterate over each folder and count the number of images\n",
        "image_counts = {}\n",
        "for folder in os.listdir(dataset_test):\n",
        "    folder_path = os.path.join(dataset_test, folder)\n",
        "    if os.path.isdir(folder_path):\n",
        "        image_counts[folder] = count_files(folder_path)\n",
        "\n",
        "# Print the results\n",
        "total_count = 0\n",
        "for folder, count in image_counts.items():\n",
        "    print(f\"{folder}: {count} images\")\n",
        "    total_count += count\n",
        "print('Length of test dataset: {}' .format(total_count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeILc90hxaUc",
        "outputId": "77900956-e051-4ab2-a74e-d815430316ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "green-glass: 503 images\n",
            "clothes: 4268 images\n",
            "white-glass: 620 images\n",
            "brown-glass: 485 images\n",
            "metal: 615 images\n",
            "trash: 557 images\n",
            "shoes: 1581 images\n",
            "cardboard: 712 images\n",
            "plastic: 692 images\n",
            "paper: 840 images\n",
            "biological: 788 images\n",
            "battery: 756 images\n",
            "Length of training dataset: 12417\n",
            "green-glass: 126 images\n",
            "clothes: 1067 images\n",
            "white-glass: 155 images\n",
            "brown-glass: 122 images\n",
            "metal: 154 images\n",
            "trash: 140 images\n",
            "shoes: 396 images\n",
            "cardboard: 179 images\n",
            "plastic: 173 images\n",
            "paper: 210 images\n",
            "biological: 197 images\n",
            "battery: 189 images\n",
            "Length of test dataset: 3108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count files in a directory\n",
        "def count_files(directory):\n",
        "    return len([filename for filename in os.listdir(directory) if os.path.isfile(os.path.join(directory, filename))])\n",
        "\n",
        "# Iterate over each folder and count the number of images\n",
        "image_counts = {}\n",
        "for folder in os.listdir(dataset):\n",
        "    folder_path = os.path.join(dataset, folder)\n",
        "    if os.path.isdir(folder_path):\n",
        "        image_counts[folder] = count_files(folder_path)\n",
        "\n",
        "# Print the results\n",
        "total_count = 0\n",
        "for folder, count in image_counts.items():\n",
        "    print(f\"{folder}: {count} images\")\n",
        "    total_count += count\n",
        "print(total_count)"
      ],
      "metadata": {
        "id": "R7WtcIpU9FMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# useful for getting number of classes\n",
        "folders = glob('/content/drive/MyDrive/garbage_dataset2/*')\n",
        "folders\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziTjczoKOGdI",
        "outputId": "9c5f58e3-b8ba-474c-f680-94c25fb4a543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/garbage_dataset2/green-glass',\n",
              " '/content/drive/MyDrive/garbage_dataset2/clothes',\n",
              " '/content/drive/MyDrive/garbage_dataset2/white-glass',\n",
              " '/content/drive/MyDrive/garbage_dataset2/brown-glass',\n",
              " '/content/drive/MyDrive/garbage_dataset2/metal',\n",
              " '/content/drive/MyDrive/garbage_dataset2/trash',\n",
              " '/content/drive/MyDrive/garbage_dataset2/shoes',\n",
              " '/content/drive/MyDrive/garbage_dataset2/cardboard',\n",
              " '/content/drive/MyDrive/garbage_dataset2/plastic',\n",
              " '/content/drive/MyDrive/garbage_dataset2/paper',\n",
              " '/content/drive/MyDrive/garbage_dataset2/biological',\n",
              " '/content/drive/MyDrive/garbage_dataset2/battery']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the dataset path\n",
        "dataset_path = '/content/drive/MyDrive/garbage_dataset2'\n",
        "\n",
        "# Define paths for train and test directories\n",
        "train_dir = '/content/drive/MyDrive/garbage_dataset2/train'\n",
        "test_dir = '/content/drive/MyDrive/garbage_dataset2/test'\n",
        "\n",
        "# Create train and test directories if they don't exist\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Iterate over each folder to collect file paths and labels\n",
        "for folder in os.listdir(dataset_path):\n",
        "    folder_path = os.path.join(dataset_path, folder)\n",
        "    if os.path.isdir(folder_path):\n",
        "        file_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path)]\n",
        "        train_paths, test_paths = train_test_split(file_paths, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Copy files to train directory\n",
        "        for path in train_paths:\n",
        "            dest_path = os.path.join(train_dir, folder)\n",
        "            os.makedirs(dest_path, exist_ok=True)\n",
        "            shutil.copy(path, dest_path)\n",
        "\n",
        "        # Copy files to test directory\n",
        "        for path in test_paths:\n",
        "            dest_path = os.path.join(test_dir, folder)\n",
        "            os.makedirs(dest_path, exist_ok=True)\n",
        "            shutil.copy(path, dest_path)\n",
        "\n",
        "print(\"Dataset split into train and test directories successfully.\")\n"
      ],
      "metadata": {
        "id": "DYLevlS3zxnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# our layers - you can add more if you want\n",
        "x = Flatten()(MobileNetV2.output)\n",
        "#x = Dense(1000, activation='relu')(x)\n",
        "prediction = Dense(len(folders), activation='softmax')(x)\n",
        "# create a model object\n",
        "model = Model(inputs=MobileNetV2.input, outputs=prediction)\n",
        "# view the structure of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "1hs-Tkq4OGfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer= 'adamax',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "rIcVORPEOGjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Directory paths\n",
        "dataset_train = '/content/drive/MyDrive/garbage_dataset2/train'\n",
        "dataset_test = '/content/drive/MyDrive/garbage_dataset2/test'\n",
        "\n",
        "# ImageDataGenerator for training data with augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    brightness_range=[0.2, 0.5],\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "\n",
        "# ImageDataGenerator for test data (only rescaling)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    dataset_train,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=5,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Flow validation images in batches using test_datagen generator\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    dataset_test,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=5,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ut84bjwQOGoL",
        "outputId": "1c7b154c-1392-47e5-cf2f-9d1d2f7524ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12417 images belonging to 12 classes.\n",
            "Found 3108 images belonging to 12 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath='mymodel.h5',\n",
        "                             verbose=2, save_best_only=True)\n",
        "\n",
        "callbacks = [checkpoint]\n",
        "start = datetime.now()\n",
        "\n",
        "r = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=len(test_generator),\n",
        "    callbacks=callbacks,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "duration = datetime.now() - start\n",
        "print(\"Training completed in time: \", duration)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tarP1vtoOUPV",
        "outputId": "b7c55b3b-1b2e-41a8-b723-02e17afc7949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 2.56625, saving model to mymodel.h5\n",
            "2484/2484 - 286s - loss: 0.7162 - accuracy: 0.8007 - val_loss: 2.5663 - val_accuracy: 0.5373 - 286s/epoch - 115ms/step\n",
            "Epoch 2/50\n",
            "\n",
            "Epoch 2: val_loss improved from 2.56625 to 1.52683, saving model to mymodel.h5\n",
            "2484/2484 - 285s - loss: 0.6414 - accuracy: 0.8128 - val_loss: 1.5268 - val_accuracy: 0.6744 - 285s/epoch - 115ms/step\n",
            "Epoch 3/50\n",
            "\n",
            "Epoch 3: val_loss improved from 1.52683 to 1.00318, saving model to mymodel.h5\n",
            "2484/2484 - 280s - loss: 0.5476 - accuracy: 0.8334 - val_loss: 1.0032 - val_accuracy: 0.7487 - 280s/epoch - 113ms/step\n",
            "Epoch 4/50\n",
            "\n",
            "Epoch 4: val_loss did not improve from 1.00318\n",
            "2484/2484 - 280s - loss: 0.4748 - accuracy: 0.8562 - val_loss: 1.9038 - val_accuracy: 0.5470 - 280s/epoch - 113ms/step\n",
            "Epoch 5/50\n",
            "\n",
            "Epoch 5: val_loss did not improve from 1.00318\n",
            "2484/2484 - 282s - loss: 0.4423 - accuracy: 0.8630 - val_loss: 3.9785 - val_accuracy: 0.5692 - 282s/epoch - 114ms/step\n",
            "Epoch 6/50\n",
            "\n",
            "Epoch 6: val_loss did not improve from 1.00318\n",
            "2484/2484 - 284s - loss: 0.4038 - accuracy: 0.8765 - val_loss: 1.8484 - val_accuracy: 0.6551 - 284s/epoch - 114ms/step\n",
            "Epoch 7/50\n",
            "\n",
            "Epoch 7: val_loss did not improve from 1.00318\n",
            "2484/2484 - 282s - loss: 0.3624 - accuracy: 0.8866 - val_loss: 1.9895 - val_accuracy: 0.6100 - 282s/epoch - 114ms/step\n",
            "Epoch 8/50\n",
            "\n",
            "Epoch 8: val_loss did not improve from 1.00318\n",
            "2484/2484 - 294s - loss: 0.3352 - accuracy: 0.8951 - val_loss: 1.4079 - val_accuracy: 0.6782 - 294s/epoch - 118ms/step\n",
            "Epoch 9/50\n",
            "\n",
            "Epoch 9: val_loss did not improve from 1.00318\n",
            "2484/2484 - 288s - loss: 0.3132 - accuracy: 0.9028 - val_loss: 1.3527 - val_accuracy: 0.7355 - 288s/epoch - 116ms/step\n",
            "Epoch 10/50\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.00318\n",
            "2484/2484 - 288s - loss: 0.2896 - accuracy: 0.9050 - val_loss: 2.7621 - val_accuracy: 0.5631 - 288s/epoch - 116ms/step\n",
            "Epoch 11/50\n",
            "\n",
            "Epoch 11: val_loss did not improve from 1.00318\n",
            "2484/2484 - 283s - loss: 0.2791 - accuracy: 0.9102 - val_loss: 2.0962 - val_accuracy: 0.6187 - 283s/epoch - 114ms/step\n",
            "Epoch 12/50\n",
            "\n",
            "Epoch 12: val_loss did not improve from 1.00318\n",
            "2484/2484 - 284s - loss: 0.2691 - accuracy: 0.9158 - val_loss: 1.5555 - val_accuracy: 0.6914 - 284s/epoch - 114ms/step\n",
            "Epoch 13/50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lY460lQ4OUUo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}